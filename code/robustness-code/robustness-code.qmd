---
title: "robustness-check"
format: html
editor: visual
---

```{r}
# Loading required packages
library(purrr)    # For map_df()
library(dplyr)    # For data manipulation
library(timetk)   # For time_series_cv()
library(rsample)  # For analysis() and assessment()
library(here)
library(modelsummary)
library(tidyr)
library(ranger) 

```

Importing train/test dataset.

```{r}
train <- readRDS(here("data", "processed-data", "train-data.rds"))
test <- readRDS(here("data", "processed-data", "test-data.rds"))
```

Here, we will be testing the regression model on out test data

```{r}
# primary regression model on test data

model <- lm(Current_health_expenditure_per_capita_current_US ~ 
              GDP_growth_annual_ + 
              Income_share_held_by_lowest_20 + 
              Control_of_Corruption_Estimate + 
              Life_expectancy_at_birth_total_years + 
Unemployment_youth_total__of_total_labor_force_ages_1524_modeled_ILO_estimate + 
              Trade__of_GDP+ Foreign_direct_investment_net_inflows_BoP_current_US+ 
  Access_to_electricity_rural__of_rural_population +
  Age_dependency_ratio__of_workingage_population
  , 
            data = test)

modelsummary(
  model,
  output = here("results", "tables", "testresult.png"),
  title = "This table represents regresssion results from the test data",
  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
  notes = list(
    "Standard errors are shown in parentheses.",
    "Significance levels: * p < 0.1, ** p < 0.05, *** p < 0.01."
  )
)

```

The results from our test dataset is show moderate performance of our model. Check the manuscript for more details on the comparision between train and test results.

# cross validation

Here, we will use cross validation to test the robustness of our model. We will have 5 year initial window and 1 year test window.

```{r}


# defining the model  
model_formula <- Current_health_expenditure_per_capita_current_US ~ 
  GDP_growth_annual_ + 
  Income_share_held_by_lowest_20 + 
  Control_of_Corruption_Estimate + 
  Life_expectancy_at_birth_total_years + 
  Unemployment_youth_total__of_total_labor_force_ages_1524_modeled_ILO_estimate + 
  Trade__of_GDP + 
  Foreign_direct_investment_net_inflows_BoP_current_US + 
  Access_to_electricity_rural__of_rural_population +
  Age_dependency_ratio__of_workingage_population

#creating time-series CV splits
cv_splits <- time_series_cv(
  data = train,
  date_var = Year,
  initial = 5,  # 5-year initial window
  assess = 1,   # 1-year test window
  skip = 1,     # Move forward 1 year each split
  slice_limit = 3
)
```

```{r}

set.seed(123)

cv <- function(splits, model_formula) {
  map_df(splits, function(split) {
    train_cv <- analysis(split)
    test_cv <- assessment(split)
    
    #training model
    model <- lm(model_formula, data = train_cv)
    
    # Getting predictions and actuals
    preds <- predict(model, newdata = test_cv)
    actuals <- test_cv$Current_health_expenditure_per_capita_current_US
    
    #converting to numeric 
    preds_num <- suppressWarnings(as.numeric(preds))
    actuals_num <- suppressWarnings(as.numeric(actuals))
    
    #calculating RMSE
    if(length(preds_num) == 0 || length(actuals_num) == 0) {
      rmse_val <- NA_real_
      status <- "Empty predictions/actuals"
    } else if(any(is.na(actuals_num))) {
      valid <- !is.na(actuals_num)
      rmse_val <- sqrt(mean((preds_num[valid] - actuals_num[valid])^2, na.rm = TRUE))
      status <- "NA in actuals"
    } else {
      rmse_val <- sqrt(mean((preds_num - actuals_num)^2, na.rm = TRUE))
      status <- "Success"
    }
    
    data.frame(
      Train_Years = paste(min(train_cv$Year), max(train_cv$Year), sep = "-"),
      Test_Year = max(test_cv$Year),
      RMSE = rmse_val,
      stringsAsFactors = FALSE
    )
  })
}

#saving the results
results <- cv(cv_splits$splits, model_formula)
print(results)
```

```{r}
#view summary of CV results
cv_summary <- results %>%
  summarise(
    Mean_RMSE = mean(RMSE, na.rm = TRUE),
    SD_RMSE = sd(RMSE, na.rm = TRUE),
  )

print(cv_summary)
```

The mean RMSE is 1151 which is simillar to the RMSE found in the original result (RMSE=1213). Therefore CV estimates show that our result is quite robust with very low Standard Deviation of 74.8

We will also test if there is any other model that performs better than OLS regression. We will use Random Forest model.

# Random Forest

```{r}
rf_cv <- function(splits, model_formula) {
  map_df(splits, function(split) {
    train_cv <- analysis(split)
    test_cv <- assessment(split)
    
    # setting the target variables
    target_var <- all.vars(model_formula)[1]
    
    # checking if Year column exists
    year_col <- grep("^year$", names(train_cv), ignore.case = TRUE, value = TRUE)
    
    #selecting required variables
    required_vars <- unique(c(all.vars(model_formula), year_col))
    train_cv <- train_cv %>% 
      select(any_of(required_vars)) %>% 
      drop_na(all_of(target_var))
    
    test_cv <- test_cv %>% 
      select(any_of(required_vars)) %>% 
      drop_na(all_of(target_var))
    
    # getting years
    get_years <- function(df) {
      if (length(year_col) > 0) {
        list(
          min_year = min(df[[year_col]], na.rm = TRUE),
          max_year = max(df[[year_col]], na.rm = TRUE)
        )
      } else {
        list(min_year = NA_real_, max_year = NA_real_)
      }
    }
    
    train_years <- get_years(train_cv)
    test_year <- get_years(test_cv)$max_year
    
    #calculate mtry
    n_predictors <- length(setdiff(required_vars, c(target_var, year_col)))
    mtry_value <- max(1, min(floor(n_predictors / 3), n_predictors))
    
    #train model
    set.seed(123)
    rf_model <- ranger(
      formula = model_formula,
      data = train_cv,
      num.trees = 500,
      mtry = mtry_value,
      importance = 'permutation',
      seed = 123
    )
    
    #calculate RMSE
    preds <- predict(rf_model, data = test_cv)$predictions
    actuals <- pull(test_cv, target_var)
    rmse_val <- sqrt(mean((preds - actuals)^2, na.rm = TRUE))
    
    #return results
    tibble(
      Train_Years = if (!is.na(train_years$min_year)) {
        paste(train_years$min_year, train_years$max_year, sep = "-")
      } else {NA_character_},
      Test_Year = test_year,
      RMSE = rmse_val,
      Model = "Random Forest",
      Predictors_Used = n_predictors,
      mtry_Used = mtry_value,
      Year_Column_Found = ifelse(length(year_col) > 0, year_col, "Not found")
    )
  })
}
```

```{r}
rf_results <- rf_cv(cv_splits$splits, model_formula)
print(rf_results)
```

The results of Random Forest are also quite good. They show very low RMSE compared to the OLS Moldel.

# Random Forest on test dataset

```{r}
library(tidyverse)
library(ranger)

#preparing training data
target_var <- all.vars(model_formula)[1]
train_processed <- train %>%
  drop_na(all_of(target_var))

#calculate appropriate mtry
n_predictors <- length(all.vars(model_formula)) - 1
mtry_value <- max(1, min(floor(n_predictors / 3), n_predictors))

#training Random Forest model
set.seed(123)
rf_model <- ranger(
  formula = model_formula,
  data = train_processed,
  num.trees = 500,
  mtry = mtry_value,
  importance = 'permutation',
  seed = 123
)
```

```{r}
#processing test data
test_processed <- test %>%
  select(any_of(names(train_processed))) %>%  # keeping only columns present in training
  drop_na(all_of(target_var))  # Removing rows with NA in target
```

```{r}
# Generating predictions
test_predictions <- predict(rf_model, data = test_processed)$predictions

#creating results dataframe
test_results <- test_processed %>%
  mutate(Predicted = test_predictions,
         Residual = .data[[target_var]] - Predicted)
```

```{r}
#calculating metrics
performance_metrics <- test_results %>%
  summarise(
    RMSE = sqrt(mean(Residual^2, na.rm = TRUE)),
    MAE = mean(abs(Residual), na.rm = TRUE),
    R2 = 1 - (sum(Residual^2) / sum((.data[[target_var]] - mean(.data[[target_var]]))^2))
  )

# printing metrics
cat("\nTest Set Performance:\n")
print(performance_metrics)

```

However, the results from test dataset are not promising for our Random Forest model. So, we will stick with our OLS model.

```{r}
#viewing the predictions vs actuals
head(test_results %>% select(all_of(target_var), Predicted, Residual))
```
